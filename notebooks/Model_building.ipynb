{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7c5acb21c943459c88f6ff52d3eab9cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_297548b578ac4f4eaba2039aba1771ed","IPY_MODEL_3eccf4424fb34b98aa5ad7321e015213","IPY_MODEL_74cb788c950f4434a2de221ecac3d581"],"layout":"IPY_MODEL_c723bb38f9914e31befa8be6a8fe7360"}},"297548b578ac4f4eaba2039aba1771ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed88f2fe8f9b4837be82d03fd2cbd700","placeholder":"​","style":"IPY_MODEL_00430edf36cb44c5b10b6af71f4f282e","value":"Map: 100%"}},"3eccf4424fb34b98aa5ad7321e015213":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47cb950ffe594dec9c8a37f70047fdf1","max":14299,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ee10032174bb43a2abdc80ae4e76c4b9","value":14299}},"74cb788c950f4434a2de221ecac3d581":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb0ae55a672b434c8232e8253638de8e","placeholder":"​","style":"IPY_MODEL_ec7cd447fb4c40d681458efda3f52a81","value":" 14299/14299 [00:10&lt;00:00, 1348.18 examples/s]"}},"c723bb38f9914e31befa8be6a8fe7360":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed88f2fe8f9b4837be82d03fd2cbd700":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00430edf36cb44c5b10b6af71f4f282e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47cb950ffe594dec9c8a37f70047fdf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee10032174bb43a2abdc80ae4e76c4b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb0ae55a672b434c8232e8253638de8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec7cd447fb4c40d681458efda3f52a81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12877677,"sourceType":"datasetVersion","datasetId":8146837}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate huggingface_hub wandb dotenv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport wandb\nimport numpy as np\nimport huggingface_hub\nimport subprocess\nimport logging\nimport json\nimport pandas as pd\nimport evaluate\nfrom datasets import Dataset\nimport transformers\nfrom transformers import (\n    RobertaConfig,\n    AutoTokenizer,\n    RobertaForSequenceClassification,\n    Trainer,\n    TrainingArguments\n)\nfrom dotenv import load_dotenv\nimport torch\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef _init_logger():\n    logger = logging.getLogger(\"model_building\")\n    logger.setLevel(logging.DEBUG)\n\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n\n    file_handler = logging.FileHandler(\"errors.log\")\n    file_handler.setLevel(logging.ERROR)\n\n    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n    console_handler.setFormatter(formatter)\n    file_handler.setFormatter(formatter)\n\n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n\ndef get_tokens():\n    \"\"\"Retrieve all the necessary names, tokens and APIs from environment.\"\"\"\n    try:\n        HF_TOKEN = os.getenv(\"HF_TOKEN\")\n        wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n        project_name = os.getenv(\"WANDB_PROJECT\")\n        run_name = os.getenv(\"WANDB_RUN_NAME\")\n        artifact_name = os.getenv(\"ARTIFACT_NAME\")\n\n        _logger.debug(\"Successfully retrieved all the needed tokens.\")\n\n        return HF_TOKEN, wandb_api_key, project_name, run_name, artifact_name\n\n    except Exception as e:\n        _logger.error(\"Un unexpected error occurred while retrievening the tokens: %s\", e)\n        raise\n\ndef login(HF_TOKEN, wandb_api_key):\n    \"\"\"Automatically log in to wandb and huggingface.\"\"\"\n    try:\n        # Huggingface login\n        huggingface_hub.login(HF_TOKEN)\n        os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n\n        # Wandb login\n        wandb.login(key=wandb_api_key)\n\n        _logger.debug(\"Successfully logged in to Hugging Face and W&B.\")\n\n    except Exception as e:\n        _logger.error(\"An unexpected error occurred: %s\", e)\n        raise\n\ndef get_config(artifact_name, project_name, run_name):\n    \"\"\"Retrieve model artifact from W&B.\"\"\"\n    try:\n        run = wandb.init(project=project_name, name=run_name)\n        artifact = run.use_artifact(artifact_name, type=\"model\")\n\n        artifact.download(root=\"src/model/config\", path_prefix=\"config.json\")\n\n        with open(\"src/model/config/config.json\", \"r\") as f:\n            content = f.read()\n            config_dict = json.loads(content)\n\n        _logger.debug(\"Successfully retrieved model configuration.\")\n        return config_dict\n\n    except Exception as e:\n        _logger.error(\"An unexpected error occurred while retrieving model configuration: %s\", e)\n        raise\n\n\n\ndef load_data(file_path):\n    \"\"\"Load data from CSV file.\"\"\"\n    try:\n        df = pd.read_csv(file_path)\n        df.fillna('', inplace=True) # Fill any NaN values\n        dataset = Dataset.from_pandas(df)\n        _logger.debug('Data loaded from %s', file_path)\n        return dataset\n    except pd.errors.ParserError as e:\n        _logger.error('Failed to parse the CSV file: %s', e)\n        raise\n    except Exception as e:\n        _logger.error('An unexpected error occurred while loading the data: %s', e)\n        raise\n\ndef compute_metrics(eval_pred):\n    load_accuracy = evaluate.load(\"accuracy\")\n    load_f1 = evaluate.load(\"f1\")\n\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n\n    return {\"accuracy\": accuracy, \"f1\": f1}\n\ndef save_model(model, file_path):\n    \"\"\"Saves model to a file.\"\"\"\n    try:\n        model.save_pretrained(file_path)\n\n        _logger.debug(\"Successfully saved model to %s\", file_path)\n\n    except Exception as e:\n        _logger.error(\"An unexpected error occurred while saving the model: %s\", e)\n        raise\n\ndef main():\n\n    # Load environment variables\n    load_dotenv()\n\n    # Clear GPU cache and set reduced precision for matmul operations\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        # Reduce matmul precision to save memory\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n        # Set reduced precision for matrix multiplications\n        torch.set_float32_matmul_precision('medium')\n\n    try:\n\n        # Load training set\n        train_dataset = load_data(\"/kaggle/input/youtube-train-csv/train.csv\")\n\n        # Retrieve necessary tokens and names\n        HF_TOKEN, wandb_api_key, project_name, run_name, artifact_name = get_tokens()\n\n        login(HF_TOKEN, wandb_api_key)\n\n        # Retrieve model configuration\n        config_dict = get_config(artifact_name, project_name, run_name)\n\n        # Initialize model\n        _logger.debug(\"Starting initialization\")\n        model = RobertaForSequenceClassification.from_pretrained(\n            \"roberta-base\",\n            num_labels=3\n        )\n        _logger.debug(\"Model has been initialized.\")\n\n        # Initialize tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n        _logger.debug(\"Tokenizer has been initialized.\")\n\n        # Tokenize dataset\n        def tokenize_function(examples):\n            return tokenizer(examples[\"text\"],\n                            truncation=True,\n                            padding=\"max_length\",\n                            max_length=512\n                            )\n\n        encoded = train_dataset.map(tokenize_function)\n        _logger.debug(\"Dataset has been tokenized.\")\n\n        # Get parameters from config\n        lr = config_dict.get(\"learning_rate\", 5e-5)\n        batch_size = config_dict.get(\"per_device_train_batch_size\", 32)\n\n        # Setup training arguments with memory optimizations\n        training_args = TrainingArguments(\n            output_dir=\"roberta_model\",\n            num_train_epochs=1,\n            per_device_train_batch_size=batch_size,\n            learning_rate=lr,\n            logging_steps=10,\n            save_strategy=\"epoch\",\n            eval_strategy=\"no\",\n            fp16=True,  # Use mixed precision training\n            dataloader_pin_memory=False,\n            remove_unused_columns=True,\n            dataloader_num_workers=0,\n            save_total_limit=1,\n            optim=\"adamw_torch\"\n        )\n\n        # Initialize trainer\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=encoded,\n            compute_metrics=compute_metrics\n        )\n        _logger.debug(\"Trainer has been initialized.\")\n\n        # Train model\n        trainer.train()\n        _logger.debug(\"Model has been trained.\")\n\n        # Clear cache before saving\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        # Save model\n        save_model(model, \"roberta_model\")\n\n    except Exception as e:\n        # Clear GPU cache in case of error\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        _logger.error(\"Failed to complete model building: %s\", e)\n        print(f\"Error: {e}\")\n\n_init_logger()\n_logger = logging.getLogger(\"model_building\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"id":"125sxCibgRKB","outputId":"146e4e8a-c658-44a0-c1f2-6f6795a58cd2","trusted":true,"execution":{"iopub.status.busy":"2025-08-26T18:02:47.633433Z","iopub.execute_input":"2025-08-26T18:02:47.634051Z","iopub.status.idle":"2025-08-26T18:15:24.927048Z","shell.execute_reply.started":"2025-08-26T18:02:47.634018Z","shell.execute_reply":"2025-08-26T18:15:24.926525Z"}},"outputs":[],"execution_count":null}]}